{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as np, random\n",
    "from jax import grad, jit, vmap\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Download and preprocess the Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_text = jax.tree_map(lambda x: x.decode(\"utf-8\"), jax.io.read_file(url))\n",
    "vocab = sorted(set(shakespeare_text))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "shakespeare_text = np.array([char_to_idx[char] for char in shakespeare_text], dtype=np.int32)\n",
    "\n",
    "# Define the Transformer model\n",
    "class Transformer(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(vocab_size=self.vocab_size, features=self.hidden_dim)\n",
    "        self.transformer_blocks = [nn.TransformerBlock(d_model=self.hidden_dim, n_heads=self.num_heads)\n",
    "                                   for _ in range(self.num_layers)]\n",
    "        self.dense = nn.Dense(features=self.vocab_size)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        for block in self.transformer_blocks:\n",
    "            embedded = block(embedded)\n",
    "        logits = self.dense(embedded)\n",
    "        return logits\n",
    "\n",
    "# Define training functions\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    return -np.mean(np.sum(nn.log_softmax(logits) * targets, axis=-1))\n",
    "\n",
    "def compute_metrics(logits, targets):\n",
    "    loss = cross_entropy_loss(logits, targets)\n",
    "    accuracy = np.mean(np.argmax(logits, axis=-1) == np.argmax(targets, axis=-1))\n",
    "    return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, batch['inputs'])\n",
    "        loss = cross_entropy_loss(logits, batch['targets'])\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = grad(loss_fn)\n",
    "    grads, logits = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits, batch['targets'])\n",
    "    return state, metrics\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Create and initialize the model and optimizer\n",
    "rng = random.PRNGKey(0)\n",
    "input_shape = (batch_size, shakespeare_text.shape[0] // batch_size)\n",
    "model = Transformer(hidden_dim=hidden_dim, num_heads=num_heads, num_layers=num_layers, vocab_size=len(vocab))\n",
    "params = model.init(rng, random.PRNGKey(1), inputs=np.ones(input_shape, dtype=np.int32))\n",
    "optimizer = optax.adam(learning_rate).create(params)\n",
    "\n",
    "# Prepare the data in batches\n",
    "def generate_batches(text, batch_size):\n",
    "    for i in range(0, len(text) - batch_size, batch_size):\n",
    "        inputs = text[i:i+batch_size]\n",
    "        targets = text[i+1:i+batch_size+1]\n",
    "        yield {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "batches = list(generate_batches(shakespeare_text, batch_size))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    state = train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=optimizer.target, tx=optax.adam(learning_rate))\n",
    "    \n",
    "    for batch in batches:\n",
    "        state, metrics = train_step(state, batch)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {metrics['loss']}, Accuracy: {metrics['accuracy']}\")\n",
    "\n",
    "# Save the trained model\n",
    "jax.tree_map(lambda x: x.block_until_ready(), state.params)\n",
    "model.save_pretrained(\"transformer_shakespeare_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c6f1dfc3191dacb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
